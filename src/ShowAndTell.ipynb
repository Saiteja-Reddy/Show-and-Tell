{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "from generator import Generator\n",
    "from models import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set Tensorflow backend to avoid full GPU pre-loading\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config = config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Generator\n"
     ]
    }
   ],
   "source": [
    "print \"Loading Generator\"\n",
    "# Hardcoding because codes will mostly run from my account.\n",
    "# Also adding sys.argv stuff with defaults is a pain.\n",
    "gen = Generator(dataset_directory = '/home/siddhartha.l/Show-and-Tell/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model (This will take while)\n"
     ]
    }
   ],
   "source": [
    "print \"Generating model (This will take while)\"\n",
    "model = get_model_with_attention(\n",
    "            gen.img_feature_size, \n",
    "            gen.vocab_size, \n",
    "            gen.max_token_len, \n",
    "            embedding_dim = gen.embedding_size)\n",
    "model.compile('adamax', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch 1/10\n",
      "632/632 [==============================] - 316s - loss: 1.1617 - acc: 0.7968   \n",
      "Epoch 2/10\n",
      "632/632 [==============================] - 315s - loss: 0.8737 - acc: 0.8215   \n",
      "Epoch 3/10\n",
      "632/632 [==============================] - 315s - loss: 0.7721 - acc: 0.8322   \n",
      "Epoch 4/10\n",
      "632/632 [==============================] - 311s - loss: 0.6852 - acc: 0.8427   \n",
      "Epoch 5/10\n",
      "632/632 [==============================] - 310s - loss: 0.6241 - acc: 0.8508   \n",
      "Epoch 6/10\n",
      "632/632 [==============================] - 309s - loss: 0.5635 - acc: 0.8603   \n",
      "Epoch 7/10\n",
      "632/632 [==============================] - 314s - loss: 0.5073 - acc: 0.8696   \n",
      "Epoch 8/10\n",
      "632/632 [==============================] - 312s - loss: 0.4700 - acc: 0.8764   \n",
      "Epoch 9/10\n",
      "632/632 [==============================] - 315s - loss: 0.4334 - acc: 0.8838   \n",
      "Epoch 10/10\n",
      "632/632 [==============================] - 313s - loss: 0.3958 - acc: 0.8917   \n",
      "Saving ~models/attention_flickr_epoch_10.h5\n",
      "Epoch 1/10\n",
      "632/632 [==============================] - 310s - loss: 0.3656 - acc: 0.8986   \n",
      "Epoch 2/10\n",
      "632/632 [==============================] - 310s - loss: 0.3469 - acc: 0.9023   \n",
      "Epoch 3/10\n",
      "632/632 [==============================] - 310s - loss: 0.3261 - acc: 0.9071   \n",
      "Epoch 4/10\n",
      "632/632 [==============================] - 320s - loss: 0.2933 - acc: 0.9154   \n",
      "Epoch 5/10\n",
      "632/632 [==============================] - 339s - loss: 0.2833 - acc: 0.9177   \n",
      "Epoch 6/10\n",
      "632/632 [==============================] - 332s - loss: 0.2688 - acc: 0.9211   \n",
      "Epoch 7/10\n",
      "632/632 [==============================] - 343s - loss: 0.2466 - acc: 0.9272   \n",
      "Epoch 8/10\n",
      "632/632 [==============================] - 347s - loss: 0.2405 - acc: 0.9284   \n",
      "Epoch 9/10\n",
      "632/632 [==============================] - 330s - loss: 0.2294 - acc: 0.9314   \n",
      "Epoch 10/10\n",
      "632/632 [==============================] - 313s - loss: 0.2156 - acc: 0.9352   \n",
      "Saving ~models/attention_flickr_epoch_20.h5\n",
      "Epoch 1/10\n",
      "632/632 [==============================] - 314s - loss: 0.2072 - acc: 0.9374   \n",
      "Epoch 2/10\n",
      "632/632 [==============================] - 313s - loss: 0.2001 - acc: 0.9393   \n",
      "Epoch 3/10\n",
      "632/632 [==============================] - 313s - loss: 0.1947 - acc: 0.9406   \n",
      "Epoch 4/10\n",
      "632/632 [==============================] - 314s - loss: 0.1840 - acc: 0.9438   \n",
      "Epoch 5/10\n",
      "632/632 [==============================] - 316s - loss: 0.1806 - acc: 0.9445   \n",
      "Epoch 6/10\n",
      "632/632 [==============================] - 311s - loss: 0.1716 - acc: 0.9473   \n",
      "Epoch 7/10\n",
      "632/632 [==============================] - 309s - loss: 0.1653 - acc: 0.9489   \n",
      "Epoch 8/10\n",
      "632/632 [==============================] - 308s - loss: 0.1640 - acc: 0.9491   \n",
      "Epoch 9/10\n",
      "632/632 [==============================] - 308s - loss: 0.1586 - acc: 0.9506   \n",
      "Epoch 10/10\n",
      "632/632 [==============================] - 308s - loss: 0.1549 - acc: 0.9516   \n",
      "Saving ~models/attention_flickr_epoch_30.h5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't decrement id ref count (unable to close file, errno = 122, error message = 'Disk quota exceeded')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fa0681396784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Saving ~models/attention_flickr_epoch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/siddhartha.l/Show-and-Tell/data/models/attention_flickr_epoch_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/siddhartha.l/.local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \"\"\"\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/siddhartha.l/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    175\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                             \u001b[0mparam_dset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/siddhartha.l/.local/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/siddhartha.l/.local/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                         \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5i.pyx\u001b[0m in \u001b[0;36mh5py.h5i.dec_ref\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't decrement id ref count (unable to close file, errno = 122, error message = 'Disk quota exceeded')"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "print \"Start training\"\n",
    "# Save model for every 10 epochs\n",
    "prev_loss = 10\n",
    "for i in range(1,10):\n",
    "    hist = model.fit_generator(\n",
    "                gen.pullData(),\n",
    "                epochs=10,\n",
    "                steps_per_epoch=int(gen.training_samples_count / batch_size), \n",
    "                shuffle = True, verbose = 1#, callbacks = [checkpointer]\n",
    "                )\n",
    "    if hist.history['loss'][-1] < prev_loss:\n",
    "        prev_loss = hist.history['loss'][-1]\n",
    "        print 'Saving ~models/attention_flickr_epoch_' + str(i*10) + '.h5'\n",
    "        model.save('/home/siddhartha.l/Show-and-Tell/data/models/attention_flickr_epoch_' + str(i*10) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/att_tmp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously saved model if necessary\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('../data/models/attention_flickr_epoch_20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For runtime testing only ###\n",
    "\n",
    "# from process_images import *\n",
    "\n",
    "base_model = InceptionV3(weights = 'imagenet', include_top = True, input_shape = (299, 299, 3))\n",
    "img_model =  Model(\n",
    "        input = base_model.input,\n",
    "        outputs = [base_model.get_layer('mixed10').output])\n",
    "target_size = (299, 299)\n",
    "print img_model.output_shape\n",
    "output_shape = img_model.output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = ['2021613437_d99731f986.jpg']\n",
    "dataset_directory = '../data/flicker8k'\n",
    "\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "\n",
    "preprocessed_images = []\n",
    "# Remove repeating filenames (if any)\n",
    "# image_filenames = list(set(image_filenames))\n",
    "number_of_images = len(image_filenames)\n",
    "img_input = []\n",
    "\n",
    "# Iterate over all images and preprocess them\n",
    "for img_id, img_name in enumerate(image_filenames): # For coco make 3D array , do batch\n",
    "    img_filepath = dataset_directory + '/Flickr8k_Dataset/' + img_name\n",
    "    # Image preprocessing\n",
    "    img = image.load_img(img_filepath, target_size = target_size)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    preprocessed_images.append(np.squeeze(img))\n",
    "\n",
    "preprocessed_images = np.asarray(preprocessed_images)\n",
    "img_features = img_model.predict(preprocessed_images)\n",
    "# img_features = np.asarray([img_features])\n",
    "\n",
    "text_in = np.zeros((1,gen.max_token_len))\n",
    "text_in[0][0] = gen.token_to_id['<start>']\n",
    "\n",
    "predictions = []\n",
    "activations = []\n",
    "for arg in range(gen.max_token_len-1):\n",
    "    pred = model.predict([img_features, text_in])\n",
    "    tok = np.argmax(pred[0][arg])\n",
    "    word = gen.id_to_token[tok]\n",
    "#     if word == 'boy':\n",
    "#         att_map = model.get_layer('time_distributed_6').output\n",
    "#         att_map = K.reshape(att_map, (39, 8, 8))\n",
    "#         att_map = att_map.eval(session = get_session())\n",
    "    text_in[0][arg+1] = tok\n",
    "    if word == '<end>':\n",
    "        break\n",
    "    predictions.append(word)\n",
    "\n",
    "print ' '.join(predictions) + '.'\n",
    "# print att_map\n",
    "Image(filename= dataset_directory + '/Flickr8k_Dataset/' + image_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = ['2021613437_d99731f986.jpg']\n",
    "dataset_directory = '../data/flicker8k'\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib import pyplot\n",
    "% matplotlib inline\n",
    "\n",
    "preprocessed_images = []\n",
    "# Remove repeating filenames (if any)\n",
    "# image_filenames = list(set(image_filenames))\n",
    "number_of_images = len(image_filenames)\n",
    "img_input = []\n",
    "\n",
    "# Iterate over all images and preprocess them\n",
    "for img_id, img_name in enumerate(image_filenames): # For coco make 3D array , do batch\n",
    "    img_filepath = dataset_directory + '/Flickr8k_Dataset/' + img_name\n",
    "    # Image preprocessing\n",
    "    img = image.load_img(img_filepath, target_size = target_size)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    preprocessed_images.append(np.squeeze(img))\n",
    "\n",
    "preprocessed_images = np.asarray(preprocessed_images)\n",
    "img_features = img_model.predict(preprocessed_images)\n",
    "# img_features = np.asarray([img_features])\n",
    "\n",
    "text_in = np.zeros((1,gen.max_token_len))\n",
    "text_in[0][0] = gen.token_to_id['<start>']\n",
    "text_in[0][1] = gen.token_to_id['a']\n",
    "text_in[0][2] = gen.token_to_id['little']\n",
    "# text_in[0][3] = gen.token_to_id['and']\n",
    "# text_in[0][4] = gen.token_to_id['a']\n",
    "\n",
    "layer_name = 'time_distributed_6'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "# intermediate_output = intermediate_layer_model.predict(data)\n",
    "\n",
    "predictions = []\n",
    "activations = []\n",
    "# for arg in range(gen.max_token_len-1):\n",
    "pred = intermediate_layer_model.predict([img_features, text_in])\n",
    "#     tok = np.argmax(pred[0][arg])\n",
    "#     word = gen.id_to_token[tok]\n",
    "#     text_in[0][arg+1] = tok\n",
    "#     if word == '<end>':\n",
    "#         break\n",
    "#     predictions.append(word)\n",
    "\n",
    "# print ' '.join(predictions) + '.'\n",
    "# print att_map\n",
    "# Image(filename= dataset_directory + '/Flickr8k_Dataset/' + image_filenames[0])\n",
    "# print pred.shape\n",
    "# pyplot.subplot()\n",
    "att = pred[0,3,:].reshape((8,8), order = 'A')#, np.ones((immmg.shape[0], immmg.shape[1])))\n",
    "# att.shape\n",
    "immmg = imread(dataset_directory + '/Flickr8k_Dataset/' + image_filenames[0])\n",
    "imshow(immmg)\n",
    "imshow(att, alpha = 0.5, interpolation='bilinear', extent=[0, immmg.shape[1], immmg.shape[0], 0])\n",
    "# imshow(pred[0,2,:].reshape(8,8))\n",
    "# imshow(pred[0,1,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(pred[0,0,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(pred[0,0,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For generating test results in bulk\n",
    "\n",
    "from IPython.display import Image\n",
    "import time\n",
    "\n",
    "with open('../data/flicker8k/preprocessed/test_captions.txt') as captions_file:\n",
    "    captions = captions_file.read().split('\\n')\n",
    "    \n",
    "class Caption:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.captions = ['','','','','']\n",
    "        self.result = ''\n",
    "        \n",
    "    def add(self, caption_number, caption):\n",
    "        self.captions[caption_number] = caption\n",
    "        \n",
    "test_results = {}\n",
    "\n",
    "for caption in captions:\n",
    "    if len(caption) < 5:\n",
    "        continue\n",
    "    caption = caption.split('\\t')\n",
    "    img_name = caption[0].split('#')\n",
    "    caption_number = int(img_name[1])\n",
    "    img_name = img_name[0]\n",
    "    caption = caption[1].lower()\n",
    "    \n",
    "    try:\n",
    "        cap_obj = test_results[img_name]\n",
    "        cap_obj.add(caption_number, caption)\n",
    "    except Exception as e:\n",
    "#         print str(e)\n",
    "        feature_dataset = h5py.File('../data/flicker8k/preprocessed/test_features.h5', 'r')\n",
    "        img_features = feature_dataset[img_name]['cnn_features'][:]\n",
    "\n",
    "        # image_filenames = get_image_filenames(dataset_directory + '/' + img_list_file)\n",
    "\n",
    "        # print img_features.shape\n",
    "        features = np.array([img_features])\n",
    "\n",
    "        text_in = np.zeros((1,gen.max_token_len))\n",
    "        text_in[0][:] = np.full((gen.max_token_len,), 0)\n",
    "        text_in[0][0] = 4230\n",
    "\n",
    "        # print features,text_in\n",
    "        arr = []\n",
    "        zeros = np.array([np.zeros(512)])\n",
    "        for arg in range(gen.max_token_len-1):\n",
    "            pred = model.predict([features, zeros, text_in])\n",
    "            tok = np.argmax(pred[0][arg])\n",
    "            word = gen.id_to_token[tok]\n",
    "            text_in[0][arg+1] = tok\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            arr.append(word)\n",
    "\n",
    "        arr.append('.')\n",
    "        cap_obj = Caption(img_name)\n",
    "        cap_obj.add(caption_number, caption)\n",
    "        cap_obj.result = ' '.join(arr)\n",
    "        test_results.update({img_name: cap_obj})\n",
    "        \n",
    "        import pickle\n",
    "        pickle.dump(test_results, open('../data/flicker8k/preprocessed/test_results.p', 'wb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for importing tested results\n",
    "import pickle\n",
    "\n",
    "class Caption:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.captions = ['','','','','']\n",
    "        self.result = ''\n",
    "        \n",
    "    def add(self, caption_number, caption):\n",
    "        self.captions[caption_number] = caption\n",
    "        \n",
    "test_results = pickle.load(open('../data/flicker8k/preprocessed/test_results.p', 'rb'))\n",
    "\n",
    "for img_name in test_results:\n",
    "    ground_truth = test_results[img_name].captions\n",
    "    result = test_results[img_name].result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
